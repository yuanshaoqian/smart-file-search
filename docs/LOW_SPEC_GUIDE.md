# å¾®å‹æ¨¡å‹æ¨èæŒ‡å—
# é€‚ç”¨äºä½é…ç½®ç”µè„‘ï¼ˆ4GB-8GB RAMï¼‰

## ğŸ¯ æ¨èçš„å¾®å‹æ¨¡å‹

### â­ è¶…å¾®å‹æ¨¡å‹ï¼ˆ2GBä»¥ä¸‹ï¼‰- **å¼ºçƒˆæ¨èä½é…ç½®ç”µè„‘**

1. **Phi-2 (2.7B Q4)**
   - **æ–‡ä»¶å¤§å°**: ~1.6 GB
   - **å†…å­˜éœ€æ±‚**: 2-3 GB RAM
   - **ä¸‹è½½**:
     ```bash
     cd ~/smart-file-search/data/models
     wget -O phi-2.Q4_K_M.gguf \
       https://huggingface.co/TheBloke/phi-2-GGUF/resolve/main/phi-2.Q4_K_M.gguf
     ```
   - **é…ç½®**:
     ```yaml
     ai:
       enabled: true
       model_path: "data/models/phi-2.Q4_K_M.gguf"
       context_size: 1024  # é™ä½ä¸Šä¸‹æ–‡é•¿åº¦
       max_tokens: 256
     ```
   - **ç‰¹ç‚¹**: å¾®è½¯å‡ºå“ï¼Œè´¨é‡ä¼˜ç§€ï¼Œé€Ÿåº¦æå¿«

2. **TinyLlama (1.1B Q4)**
   - **æ–‡ä»¶å¤§å°**: ~0.6 GB
   - **å†…å­˜éœ€æ±‚**: 1-2 GB RAM
   - **ä¸‹è½½**:
     ```bash
     wget -O tinyllama-1.1b.Q4_K_M.gguf \
       https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf
     ```
   - **é…ç½®**:
     ```yaml
     ai:
       enabled: true
       model_path: "data/models/tinyllama-1.1b.Q4_K_M.gguf"
       context_size: 1024
       max_tokens: 256
     ```
   - **ç‰¹ç‚¹**: è¶…è½»é‡çº§ï¼Œæå¿«é€Ÿåº¦

3. **Qwen 1.5 0.5B (ä¸­æ–‡ä¼˜åŒ–)**
   - **æ–‡ä»¶å¤§å°**: ~0.4 GB
   - **å†…å­˜éœ€æ±‚**: 1 GB RAM
   - **ä¸‹è½½**:
     ```bash
     wget -O qwen-0.5b.Q4_K_M.gguf \
       https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/resolve/main/qwen1.5-0.5b-chat.Q4_K_M.gguf
     ```
   - **é…ç½®**:
     ```yaml
     ai:
       enabled: true
       model_path: "data/models/qwen-0.5b.Q4_K_M.gguf"
       context_size: 1024
       max_tokens: 256
     ```
   - **ç‰¹ç‚¹**: ä¸­æ–‡æ”¯æŒæœ€ä½³ï¼Œæå°ä½“ç§¯

### ğŸ’ª å°å‹æ¨¡å‹ï¼ˆ2-4GBï¼‰- æ¨è

1. **Mistral 7B Q4 (æ¨è)**
   - **æ–‡ä»¶å¤§å°**: ~4.1 GB
   - **å†…å­˜éœ€æ±‚**: 5-6 GB RAM
   - **ä¸‹è½½**:
     ```bash
     wget -O mistral-7b.Q4_K_M.gguf \
       https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf
     ```
   - **ç‰¹ç‚¹**: è´¨é‡é«˜ï¼Œé€Ÿåº¦å¿«

2. **Qwen 1.5 1.8B (ä¸­æ–‡)**
   - **æ–‡ä»¶å¤§å°**: ~1.2 GB
   - **å†…å­˜éœ€æ±‚**: 2-3 GB RAM
   - **ä¸‹è½½**:
     ```bash
     wget -O qwen-1.8b.Q4_K_M.gguf \
       https://huggingface.co/Qwen/Qwen1.5-1.8B-Chat-GGUF/resolve/main/qwen1.5-1.8b-chat.Q4_K_M.gguf
     ```
   - **ç‰¹ç‚¹**: ä¸­æ–‡æ”¯æŒå¥½ï¼Œä½“ç§¯å°

## ğŸ”§ ä½é…ç½®ç”µè„‘ä¼˜åŒ–å»ºè®®

### é…ç½®ä¼˜åŒ–ï¼ˆconfig.yamlï¼‰

```yaml
# AI é…ç½®ä¼˜åŒ–
ai:
  enabled: true
  model_path: "data/models/phi-2.Q4_K_M.gguf"  # ä½¿ç”¨å¾®å‹æ¨¡å‹
  
  # é™ä½ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆå‡å°‘å†…å­˜å ç”¨ï¼‰
  context_size: 1024  # é»˜è®¤2048ï¼Œä½é…å»ºè®®512-1024
  
  # å‡å°‘ç”Ÿæˆé•¿åº¦
  max_tokens: 256  # é»˜è®¤512ï¼Œä½é…å»ºè®®128-256
  
  # é™ä½æ¸©åº¦ï¼ˆæé«˜ç¡®å®šæ€§ï¼Œå‡å°‘è®¡ç®—ï¼‰
  temperature: 0.1

# ç´¢å¼•ä¼˜åŒ–
index:
  # å‡å°‘å¹¶å‘çº¿ç¨‹æ•°
  max_file_size: 52428800  # 50MBï¼ˆé»˜è®¤100MBï¼‰
  
  # å‡å°‘ç´¢å¼•é¢‘ç‡
  update_interval: 600  # 10åˆ†é’Ÿï¼ˆé»˜è®¤5åˆ†é’Ÿï¼‰

# GUI ä¼˜åŒ–
gui:
  # å‡å°‘ç»“æœæ˜¾ç¤ºæ•°é‡
  max_results: 200  # é»˜è®¤500
  
  # ç¦ç”¨é¢„è§ˆï¼ˆå¯é€‰ï¼‰
  preview_max_lines: 10  # é»˜è®¤20

# é«˜çº§ä¼˜åŒ–
advanced:
  # å‡å°‘çº¿ç¨‹æ•°
  parser_threads: 2  # é»˜è®¤4
  indexer_threads: 1  # é»˜è®¤2
  
  # å‡å°‘ç¼“å­˜
  cache_size_mb: 50  # é»˜è®¤100
```

### å†…å­˜ä¼˜åŒ–æŠ€å·§

1. **ä½¿ç”¨æ›´ä½çš„é‡åŒ–ç‰ˆæœ¬**
   - Q4_K_M (æ¨è): è´¨é‡ä¸é€Ÿåº¦å¹³è¡¡
   - Q4_K_S: æ›´å°ä½“ç§¯ï¼Œç•¥ä½è´¨é‡
   - Q3_K_M: æœ€å°ä½“ç§¯ï¼Œè´¨é‡è¾ƒä½

2. **è°ƒæ•´ä¸Šä¸‹æ–‡é•¿åº¦**
   ```yaml
   context_size: 512  # æä½å†…å­˜ï¼ˆ<4GBï¼‰
   context_size: 1024  # ä½å†…å­˜ï¼ˆ4-6GBï¼‰
   context_size: 2048  # ä¸­ç­‰å†…å­˜ï¼ˆ8GB+ï¼‰
   ```

3. **ç¦ç”¨AIåŠŸèƒ½ï¼ˆä»…ä½¿ç”¨ç´¢å¼•ï¼‰**
   ```yaml
   ai:
     enabled: false  # çº¯ç´¢å¼•æœç´¢ï¼Œæ— AIç†è§£
   ```

## ğŸ’» ä¸åŒé…ç½®æ¨è

### æä½é…ç½®ï¼ˆ2-4GB RAMï¼‰
- **æ¨¡å‹**: TinyLlama 1.1B æˆ– Qwen 0.5B
- **é…ç½®**:
  ```yaml
  ai:
    enabled: true
    model_path: "data/models/tinyllama-1.1b.Q4_K_M.gguf"
    context_size: 512
    max_tokens: 128
  ```

### ä½é…ç½®ï¼ˆ4-6GB RAMï¼‰
- **æ¨¡å‹**: Phi-2 2.7B æˆ– Qwen 1.8B
- **é…ç½®**:
  ```yaml
  ai:
    enabled: true
    model_path: "data/models/phi-2.Q4_K_M.gguf"
    context_size: 1024
    max_tokens: 256
  ```

### ä¸­ç­‰é…ç½®ï¼ˆ8-12GB RAMï¼‰
- **æ¨¡å‹**: Mistral 7B æˆ– Qwen 7B
- **é…ç½®**:
  ```yaml
  ai:
    enabled: true
    model_path: "data/models/mistral-7b.Q4_K_M.gguf"
    context_size: 2048
    max_tokens: 512
  ```

### é«˜é…ç½®ï¼ˆ16GB+ RAMï¼‰
- **æ¨¡å‹**: Llama 2 13B æˆ–æ›´å¤§
- **é…ç½®**: ä½¿ç”¨é»˜è®¤é…ç½®å³å¯

## ğŸš€ å¿«é€Ÿå¼€å§‹ï¼ˆä½é…ç‰ˆï¼‰

```bash
# 1. åˆ›å»ºæ¨¡å‹ç›®å½•
cd ~/smart-file-search
mkdir -p data/models

# 2. ä¸‹è½½ Phi-2ï¼ˆæ¨èä½é…ï¼‰
cd data/models
wget -O phi-2.Q4_K_M.gguf \
  https://huggingface.co/TheBloke/phi-2-GGUF/resolve/main/phi-2.Q4_K_M.gguf

# 3. ä¿®æ”¹é…ç½®
cd ~/smart-file-search
nano config.yaml
# ä¿®æ”¹ ai.enabled: true
# ä¿®æ”¹ ai.model_path: "data/models/phi-2.Q4_K_M.gguf"
# ä¿®æ”¹ ai.context_size: 1024
# ä¿®æ”¹ ai.max_tokens: 256

# 4. å®‰è£…AIä¾èµ–
pip install llama-cpp-python

# 5. å¯åŠ¨ç¨‹åº
./start.sh
```

## âš¡ æ€§èƒ½å¯¹æ¯”

| æ¨¡å‹ | å¤§å° | å†…å­˜ | é€Ÿåº¦ | è´¨é‡ | é€‚ç”¨åœºæ™¯ |
|------|------|------|------|------|---------|
| TinyLlama 1.1B | 0.6GB | 1-2GB | âš¡âš¡âš¡âš¡âš¡ | â­â­ | æä½é…ç½® |
| Qwen 0.5B | 0.4GB | 1GB | âš¡âš¡âš¡âš¡âš¡ | â­â­ | ä¸­æ–‡æä½é… |
| Phi-2 2.7B | 1.6GB | 2-3GB | âš¡âš¡âš¡âš¡ | â­â­â­â­ | ä½é…ç½®æ¨è |
| Qwen 1.8B | 1.2GB | 2-3GB | âš¡âš¡âš¡âš¡ | â­â­â­â­ | ä¸­æ–‡ä½é… |
| Mistral 7B | 4.1GB | 5-6GB | âš¡âš¡âš¡ | â­â­â­â­â­ | ä¸­é…ç½® |
| Llama 2 7B | 4.0GB | 6GB | âš¡âš¡âš¡ | â­â­â­â­â­ | ä¸­é…ç½® |

## ğŸ” å¸¸è§é—®é¢˜

**Q: æˆ‘çš„ç”µè„‘åªæœ‰4GBå†…å­˜ï¼Œèƒ½ç”¨å—ï¼Ÿ**
A: å¯ä»¥ï¼ä½¿ç”¨ TinyLlama 1.1B æˆ– Qwen 0.5Bï¼Œè®¾ç½® context_size=512

**Q: ä¸æƒ³ç”¨AIï¼Œåªæƒ³å¿«é€Ÿæœç´¢æ–‡ä»¶ï¼Ÿ**
A: è®¾ç½® `ai.enabled: false`ï¼Œä»…ä½¿ç”¨ç´¢å¼•åŠŸèƒ½ï¼ˆç±»ä¼¼Everythingï¼‰

**Q: å¦‚ä½•æŸ¥çœ‹å†…å­˜å ç”¨ï¼Ÿ**
A: å¯åŠ¨ç¨‹åºåï¼Œä¾§è¾¹æ ä¼šæ˜¾ç¤º"AIçŠ¶æ€"å’Œå†…å­˜å ç”¨

**Q: æ¨¡å‹åŠ è½½å¤±è´¥ï¼Ÿ**
A: æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å®Œæ•´ä¸‹è½½ï¼Œè·¯å¾„æ˜¯å¦æ­£ç¡®

## ğŸ“š èµ„æºé“¾æ¥

- **Hugging Face GGUF æ¨¡å‹åº“**: https://huggingface.co/models?search=gguf
- **llama.cpp æ–‡æ¡£**: https://github.com/ggerganov/llama.cpp
- **é‡åŒ–è¯´æ˜**: https://github.com/ggerganov/llama.cpp#quantization

---

**æ¨è**: ä½é…ç½®ç”µè„‘é¦–é€‰ **Phi-2 2.7B** æˆ– **Qwen 1.8B**ï¼Œå¹³è¡¡æ€§èƒ½å’Œè´¨é‡ï¼
